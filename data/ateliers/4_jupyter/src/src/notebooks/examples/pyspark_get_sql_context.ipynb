{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyspark local mode\n",
    "\n",
    "Auteur : Biard Antoine\n",
    "\n",
    "\n",
    "Date : 11/10/2020\n",
    "\n",
    "Dans ce notebook, nous présentons comment lancer un cluster pyspark en mode local, et présentons les helpers du repository.\n",
    "\n",
    "## TLDR\n",
    "\n",
    "Utiliser simplement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[4] appName=notebooks>\n",
      "<pyspark.sql.context.SQLContext object at 0x7f6c595df790>\n"
     ]
    }
   ],
   "source": [
    "from utils.spark import get_spark_sql_context, get_spark_context\n",
    "\n",
    "sc = get_spark_context()\n",
    "print(sc)\n",
    "sql_context = get_spark_sql_context()\n",
    "print(sql_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création d'un SparkContext\n",
    "\n",
    "Source : [guide](https://docs.faculty.ai/how_to/spark/local_spark.html) de [Faculty.ai](faculty.ai).\n",
    "\n",
    "D'abord nous précisons à Pyspark quel python utiliser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création d'une configuration en mode local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "number_cores = 8\n",
    "memory_gb = 24\n",
    "\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setMaster('local[{}]'.format(number_cores))\n",
    "        .set('spark.driver.memory', '{}g'.format(memory_gb))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création d'un contexte spark à partir de la configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il ne peut y avoir qu'un seul context par configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le chargement d'un nouveau context fails, l'erreur renvoyée par pyspark : \n",
      "\n",
      "Cannot run multiple SparkContexts at once; existing SparkContext(app=notebooks, master=local[8]) created by __init__ at <ipython-input-5-c9f39e965e09>:1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    sc_same_conf = pyspark.SparkContext(conf=conf)\n",
    "except ValueError as ve:\n",
    "    print(\"Le chargement d'un nouveau context fails, l'erreur renvoyée par pyspark : \")\n",
    "    print()\n",
    "    print(ve)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pyspark.SparkContext` possède une méthode de classe `getOrCreate` permettant de récupérer un context à partir d'une configuration sans ce problème.\n",
    "\n",
    "L'implémentation derrière est faite sous la forme d'un [singleton](https://github.com/apache/spark/blob/master/python/pyspark/context.py#L364).\n",
    "\n",
    "Id python du premier SparkContext :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140104062323152"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Id python de `sc._active_spark_context` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140104062323152"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(sc._active_spark_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Id python du SparkContext en utilisant `getOrCreate` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140104062323152"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc_from_get_or_create =  pyspark.SparkContext.getOrCreate(conf)\n",
    "\n",
    "id(sc_from_get_or_create)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tous ces objets sont les mêmes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions dans `utils.spark`\n",
    "\n",
    "Afin d'éviter d'écrire toutes ces lignes, le module `utils.spark` contient quelques helpers.\n",
    "\n",
    "Le premier permet de récupérer le spark context actif :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[8] appName=notebooks>\n",
      "140104062323152\n",
      "<SparkContext master=local[8] appName=notebooks>\n",
      "140104062323152\n"
     ]
    }
   ],
   "source": [
    "from utils.spark import get_spark_context\n",
    "\n",
    "sc_1 = get_spark_context()\n",
    "print(sc_1)\n",
    "print(id(sc_1))\n",
    "sc_2 = get_spark_context()\n",
    "print(sc_2)\n",
    "print(id(sc_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le second un sql context de la même manière :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x7f6c84ddcca0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.spark import get_spark_sql_context\n",
    "\n",
    "sqc = get_spark_sql_context()\n",
    "sqc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
